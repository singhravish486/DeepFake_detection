# =============================================================================
# HYBRID CNN + ViT DEEPFAKE DETECTOR FOR REAL DATASETS
# DeepFake Detection in Aerial Images Using Explainable AI
# =============================================================================

# =============================================================================
# CELL 1: LOCAL ENVIRONMENT SETUP AND PACKAGES
# =============================================================================
# For local training, install packages using:
# pip install -r requirements.txt

# Memory and performance optimization imports
import os
import gc
import psutil
import threading
import time
from contextlib import contextmanager

# GPU monitoring (if nvidia-ml-py3 is installed)
try:
    import pynvml
    pynvml.nvmlInit()
    GPU_MONITORING = True
    print("‚úÖ GPU monitoring enabled")
except (ImportError, Exception) as e:
    GPU_MONITORING = False
    print("‚ö†Ô∏è GPU monitoring not available - continuing without monitoring")
    print(f"   Reason: {type(e).__name__}")

# Verify critical installations
import sys
try:
    import transformers
    print(f"‚úÖ Transformers version: {transformers.__version__}")
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è Transformers not available - will use CNN-only model")
    TRANSFORMERS_AVAILABLE = False

try:
    import tensorflow as tf
    print(f"‚úÖ TensorFlow version: {tf.__version__}")
except ImportError:
    print("‚ùå TensorFlow installation failed!")
    sys.exit(1)

try:
    import shap
    print("‚úÖ SHAP installed successfully")
except ImportError:
    print("‚ö†Ô∏è SHAP not available - explainability features limited")

print("üéØ Package installation check complete!")

# =============================================================================
# CELL 2: IMPORT ALL LIBRARIES
# =============================================================================
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cv2
import os
import shutil
import zipfile
import random
from PIL import Image
import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc

# Deep Learning Libraries
from tensorflow.keras.layers import (Input, Dense, Concatenate, Dropout, 
                                   GlobalAveragePooling2D, BatchNormalization)
from tensorflow.keras.models import Model
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.optimizers import Adam

# Transformers for ViT
from transformers import TFViTModel, ViTImageProcessor

# Explainable AI (optional)
try:
    import shap
    SHAP_AVAILABLE = True
    print("‚úÖ SHAP loaded successfully")
except ImportError as e:
    SHAP_AVAILABLE = False
    print(f"‚ö†Ô∏è SHAP not available: {e}")
    print("üí° Grad-CAM will still work for explainability")

from tensorflow.keras.utils import plot_model

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
random.seed(42)

print("‚úÖ All libraries imported successfully!")
print(f"TensorFlow version: {tf.__version__}")

# =============================================================================
# CELL 3: OPTIMIZED GPU SETUP FOR GTX 1650
# =============================================================================
class GPUMonitor:
    """Monitor GPU usage and memory for GTX 1650 optimization"""
    def __init__(self):
        self.monitoring = GPU_MONITORING
        
    def get_gpu_info(self):
        """Get current GPU memory usage"""
        if not self.monitoring:
            return None
        try:
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            return {
                'used': mem_info.used // 1024**2,  # MB
                'total': mem_info.total // 1024**2,  # MB
                'free': mem_info.free // 1024**2   # MB
            }
        except:
            return None
    
    def print_gpu_status(self):
        """Print current GPU status"""
        info = self.get_gpu_info()
        if info:
            usage_percent = (info['used'] / info['total']) * 100
            print(f"üîß GPU Memory: {info['used']}MB/{info['total']}MB ({usage_percent:.1f}%)")

def setup_gpu_optimized():
    """Configure GPU settings optimized for GTX 1650 (4GB VRAM)"""
    print("üöÄ Setting up GPU for GTX 1650 optimization...")
    
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            # Critical for GTX 1650: Enable memory growth to prevent allocation errors
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
                
            # Set memory limit for GTX 1650 (reserve some VRAM for system)
            tf.config.experimental.set_memory_limit(gpus[0], 3584)  # 3.5GB limit
            
            print(f"‚úÖ GPU configured: {len(gpus)} GPU(s) available")
            print(f"GPU Name: {gpus[0].name}")
            
        except RuntimeError as e:
            print(f"‚ùå GPU setup error: {e}")
            print("üí° Try restarting Python kernel if GPU was already initialized")
    else:
        print("‚ùå No GPU available! GTX 1650 should be detected.")
        print("üí° Check CUDA installation and GPU drivers")
        return False
    
    # Enable mixed precision for GTX 1650 (significant speedup)
    policy = tf.keras.mixed_precision.Policy('mixed_float16')
    tf.keras.mixed_precision.set_global_policy(policy)
    print("‚úÖ Mixed precision enabled (FP16) - major speedup on GTX 1650")
    
    # Configure TensorFlow for optimal GTX 1650 performance
    tf.config.threading.set_intra_op_parallelism_threads(4)
    tf.config.threading.set_inter_op_parallelism_threads(2)
    
    return True

@contextmanager
def gpu_memory_monitor():
    """Context manager to monitor GPU memory usage"""
    monitor = GPUMonitor()
    print("üìä Starting GPU memory monitoring...")
    monitor.print_gpu_status()
    
    try:
        yield monitor
    finally:
        # Force garbage collection
        gc.collect()
        tf.keras.backend.clear_session()
        print("üßπ Cleaned up GPU memory")
        monitor.print_gpu_status()

# Setup GPU for local training
gpu_success = setup_gpu_optimized()
if not gpu_success:
    print("‚ö†Ô∏è Continuing with CPU training (will be very slow for 4000 images)")

# Initialize GPU monitor
gpu_monitor = GPUMonitor()

# =============================================================================
# CELL 4: LOCAL DATASET CONFIGURATION (4000 IMAGES)
# =============================================================================
def find_dataset_path():
    """Smart dataset path detection for local training"""
    
    # Common local dataset paths
    possible_paths = [
        os.path.join(os.getcwd(), 'dataset'),           # ./dataset
        os.path.join(os.getcwd(), 'data'),              # ./data
        os.path.join(os.getcwd(), '..', 'dataset'),     # ../dataset
        'D:\\dataset',                                   # Windows D: drive
        'C:\\Users\\dataset',                           # Windows Users folder
        os.path.expanduser('~/dataset'),                # User home directory
        os.path.expanduser('~/Downloads/dataset'),      # Downloads folder
    ]
    
    print("üîç Searching for dataset in common locations...")
    for path in possible_paths:
        if os.path.exists(path):
            print(f"‚úÖ Found dataset at: {path}")
            return path
    
    # Interactive path input
    print("‚ùå Dataset not found in common locations.")
    print("\nüí° Please provide your dataset path:")
    print("Example paths:")
    print("  - D:\\my_dataset")
    print("  - C:\\Users\\YourName\\dataset") 
    print("  - ./dataset (if in current directory)")
    
    while True:
        user_path = input("\nüìÅ Enter your dataset path: ").strip().strip('"')
        if os.path.exists(user_path):
            return user_path
        else:
            print(f"‚ùå Path not found: {user_path}")
            retry = input("Try again? (y/n): ").lower()
            if retry != 'y':
                break
    
    return None

# Set dataset path for local training
DATASET_PATH = r"D:\dataset"

if DATASET_PATH is None:
    print("‚ùå No valid dataset path provided!")
    print("üí° Please organize your 4000 images as:")
    print("   your_dataset/")
    print("   ‚îú‚îÄ‚îÄ real/    (2000 real images)")
    print("   ‚îî‚îÄ‚îÄ fake/    (2000 fake images)")
    exit(1)

# Dataset validation for 4000 images
print(f"\nüìä Analyzing dataset: {DATASET_PATH}")

def validate_dataset_size(path):
    """Validate dataset has adequate number of images for training"""
    real_path = os.path.join(path, 'real')
    fake_path = os.path.join(path, 'fake')
    
    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']
    
    real_count = 0
    fake_count = 0
    
    if os.path.exists(real_path):
        real_count = len([f for f in os.listdir(real_path) 
                         if any(f.lower().endswith(ext) for ext in image_extensions)])
    
    if os.path.exists(fake_path):
        fake_count = len([f for f in os.listdir(fake_path) 
                         if any(f.lower().endswith(ext) for ext in image_extensions)])
    
    total_images = real_count + fake_count
    
    print(f"üìà Dataset Analysis:")
    print(f"   Real images: {real_count}")
    print(f"   Fake images: {fake_count}")
    print(f"   Total images: {total_images}")
    
    if total_images < 1000:
        print("‚ö†Ô∏è Warning: Dataset might be too small for good performance")
    elif total_images >= 3000:
        print("‚úÖ Excellent! Large dataset will provide good training")
    
    return real_count, fake_count, total_images

real_count, fake_count, total_images = validate_dataset_size(DATASET_PATH)

# Auto-split configuration for large datasets
AUTO_SPLIT = True  # Always auto-split for local training
TRAIN_RATIO = 0.7   # 70% for training
VAL_RATIO = 0.15    # 15% for validation  
TEST_RATIO = 0.15   # 15% for testing

print(f"\nüéØ Configuration for {total_images} images:")
print(f"   Training: {int(total_images * TRAIN_RATIO)} images")
print(f"   Validation: {int(total_images * VAL_RATIO)} images")
print(f"   Testing: {int(total_images * TEST_RATIO)} images")

# =============================================================================
# CELL 5: DATASET DISCOVERY AND VALIDATION
# =============================================================================
def discover_dataset_structure(dataset_path):
    """Discover and validate dataset structure"""
    print(f"üîç Analyzing dataset structure at: {dataset_path}")
    
    if not os.path.exists(dataset_path):
        print(f"‚ùå Dataset path does not exist: {dataset_path}")
        print("Please check your dataset upload!")
        return None
    
    # Check for standard structure
    splits = ['train', 'validation', 'test']
    classes = ['real', 'fake']
    
    structure_info = {}
    has_standard_structure = True
    
    for split in splits:
        split_path = os.path.join(dataset_path, split)
        if os.path.exists(split_path):
            structure_info[split] = {}
            for class_name in classes:
                class_path = os.path.join(split_path, class_name)
                if os.path.exists(class_path):
                    # Count images
                    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']
                    images = [f for f in os.listdir(class_path) 
                             if any(f.lower().endswith(ext) for ext in image_extensions)]
                    structure_info[split][class_name] = len(images)
                    print(f"  {split}/{class_name}: {len(images)} images")
                else:
                    structure_info[split][class_name] = 0
                    has_standard_structure = False
        else:
            has_standard_structure = False
    
    if not has_standard_structure:
        print("‚ö†Ô∏è Standard structure not found. Checking for alternative structures...")
        
        # Check for flat structure (all images in subdirectories)
        subdirs = [d for d in os.listdir(dataset_path) 
                  if os.path.isdir(os.path.join(dataset_path, d))]
        
        print(f"Found subdirectories: {subdirs}")
        
        for subdir in subdirs:
            subdir_path = os.path.join(dataset_path, subdir)
            image_count = len([f for f in os.listdir(subdir_path) 
                             if any(f.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff'])])
            print(f"  {subdir}: {image_count} images")
    
    return structure_info

# Analyze your dataset
dataset_info = discover_dataset_structure(DATASET_PATH)

# =============================================================================
# CELL 6: AUTOMATIC DATASET SPLITTING (if needed)
# =============================================================================
def create_train_val_test_split_optimized(source_path, dest_path, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    """Optimized train/validation/test splits for large datasets (4000+ images)"""
    print(f"üîÑ Creating optimized train/val/test splits for large dataset...")
    
    # Create destination structure
    for split in ['train', 'validation', 'test']:
        for class_name in ['real', 'fake']:
            os.makedirs(os.path.join(dest_path, split, class_name), exist_ok=True)
    
    # Process each class with progress tracking
    total_processed = 0
    
    for class_name in ['real', 'fake']:
        source_class_path = os.path.join(source_path, class_name)
        
        if not os.path.exists(source_class_path):
            print(f"‚ö†Ô∏è Class directory not found: {source_class_path}")
            continue
        
        # Get all images (including more formats)
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']
        images = [f for f in os.listdir(source_class_path) 
                 if any(f.lower().endswith(ext) for ext in image_extensions)]
        
        print(f"üìÅ Processing {len(images)} {class_name} images...")
        
        # Stratified shuffle for better distribution
        random.shuffle(images)
        
        # Calculate split sizes
        total = len(images)
        train_size = int(total * train_ratio)
        val_size = int(total * val_ratio)
        
        # Split images
        train_images = images[:train_size]
        val_images = images[train_size:train_size + val_size]
        test_images = images[train_size + val_size:]
        
        # Optimized file copying with progress
        def copy_with_progress(image_list, split_name):
            dest_class_path = os.path.join(dest_path, split_name, class_name)
            for i, image in enumerate(image_list):
                src = os.path.join(source_class_path, image)
                dst = os.path.join(dest_class_path, image)
                shutil.copy2(src, dst)
                
                # Progress indicator for large datasets
                if (i + 1) % 100 == 0:
                    print(f"   Copied {i + 1}/{len(image_list)} {split_name} {class_name} images")
        
        # Copy files to respective splits
        copy_with_progress(train_images, 'train')
        copy_with_progress(val_images, 'validation') 
        copy_with_progress(test_images, 'test')
        
        total_processed += len(images)
        print(f"‚úÖ {class_name}: {len(train_images)} train, {len(val_images)} val, {len(test_images)} test")
    
    print(f"‚úÖ Successfully processed {total_processed} images!")
    
    # Memory cleanup after large operations
    gc.collect()

# Auto-split if needed (check for flat structure)
dataset_info = discover_dataset_structure(DATASET_PATH)

# Optimized split creation for local training
real_path = os.path.join(DATASET_PATH, 'real')
fake_path = os.path.join(DATASET_PATH, 'fake')

if os.path.exists(real_path) and os.path.exists(fake_path):
    print("üîÑ Detected flat structure (real/fake folders) - creating optimized train/val/test splits...")
    
    # Use current directory for local training (not /content)
    split_dataset_path = os.path.join(os.path.dirname(DATASET_PATH), 'dataset_split')
    
    print(f"üìÅ Creating splits in: {split_dataset_path}")
    
    with gpu_memory_monitor():
        create_train_val_test_split_optimized(
            DATASET_PATH, 
            split_dataset_path, 
            train_ratio=TRAIN_RATIO,
            val_ratio=VAL_RATIO, 
            test_ratio=TEST_RATIO
        )
    
    DATASET_PATH = split_dataset_path
    
    # Verify the split was created
    dataset_info = discover_dataset_structure(DATASET_PATH)
    print(f"‚úÖ Dataset auto-split completed! Using: {DATASET_PATH}")
else:
    print("‚ÑπÔ∏è Using existing dataset structure")

# =============================================================================
# CELL 7: OPTIMIZED DATA PREPROCESSING FOR GTX 1650 & 4000 IMAGES
# =============================================================================
class OptimizedDataPreprocessor:
    def __init__(self, image_size=(224, 224), batch_size=8):
        self.image_size = image_size
        self.batch_size = batch_size
        
        print(f"üîß Initializing data preprocessor for GTX 1650:")
        print(f"   Batch size: {batch_size} (optimized for 4GB VRAM)")
        print(f"   Image size: {image_size}")
        
        # Optimized data augmentation for training - balanced for performance
        self.train_datagen = ImageDataGenerator(
            rescale=1./255,
            rotation_range=20,           # Reduced for faster processing
            width_shift_range=0.1,       
            height_shift_range=0.1,
            horizontal_flip=True,        
            vertical_flip=True,          
            zoom_range=0.1,             # Reduced zoom range
            brightness_range=[0.9, 1.1], # Reduced brightness range
            fill_mode='nearest',
            validation_split=0.0         # We handle splits manually
        )
        
        # No augmentation for validation/test (faster loading)
        self.val_datagen = ImageDataGenerator(rescale=1./255)
        
        # Add prefetching for faster data loading
        self.prefetch_size = 2  # Prefetch 2 batches
    
    def create_generators(self, dataset_path):
        """Create optimized data generators for GTX 1650 training"""
        generators = {}
        
        print("üîß Creating optimized data generators...")
        
        for split in ['train', 'validation', 'test']:
            split_path = os.path.join(dataset_path, split)
            
            if not os.path.exists(split_path):
                print(f"‚ö†Ô∏è Split directory not found: {split_path}")
                continue
            
            if split == 'train':
                datagen = self.train_datagen
                shuffle = True
                print(f"üìä Creating training generator with augmentation...")
            else:
                datagen = self.val_datagen
                shuffle = False
                print(f"üìä Creating {split} generator without augmentation...")
            
            try:
                generator = datagen.flow_from_directory(
                    split_path,
                    target_size=self.image_size,
                    batch_size=self.batch_size,
                    class_mode='binary',
                    shuffle=shuffle,
                    seed=42
                )
                
                # Wrap with prefetch for better performance
                generators[split] = generator
                
                print(f"‚úÖ {split} generator created: {generator.samples} samples")
                print(f"   Classes found: {list(generator.class_indices.keys())}")
                
            except Exception as e:
                print(f"‚ùå Failed to create {split} generator: {e}")
        
        return generators
    
    def estimate_training_time(self, total_samples, epochs):
        """Estimate training time for GTX 1650"""
        steps_per_epoch = max(1, total_samples // self.batch_size)
        
        # GTX 1650 performance estimates (seconds per step)
        time_per_step = 0.8  # Approximate time per step with hybrid model
        
        epoch_time = steps_per_epoch * time_per_step
        total_time = epoch_time * epochs
        
        print(f"‚è±Ô∏è Training Time Estimates for GTX 1650:")
        print(f"   Steps per epoch: {steps_per_epoch}")
        print(f"   Time per epoch: ~{epoch_time/60:.1f} minutes")
        print(f"   Total training time ({epochs} epochs): ~{total_time/3600:.1f} hours")

# Optimized settings for GTX 1650 and 4000 images
BATCH_SIZE = 8   # Start with 8, can increase to 12-16 if memory allows
IMAGE_SIZE = (224, 224)  # Optimal for both CNN and ViT components

print("üéØ GTX 1650 Optimization Settings:")
print(f"   Batch Size: {BATCH_SIZE} (memory optimized)")
print(f"   Image Size: {IMAGE_SIZE}")
print(f"   Mixed Precision: Enabled")

preprocessor = OptimizedDataPreprocessor(image_size=IMAGE_SIZE, batch_size=BATCH_SIZE)
data_generators = preprocessor.create_generators(DATASET_PATH)

# Extract generators
train_gen = data_generators.get('train')
val_gen = data_generators.get('validation')
test_gen = data_generators.get('test')

if train_gen is None:
    print("‚ùå No training data found! Please check your dataset path.")
    exit(1)
else:
    print(f"‚úÖ Data generators created successfully!")
    print(f"Training samples: {train_gen.samples}")
    if val_gen:
        print(f"Validation samples: {val_gen.samples}")
    if test_gen:
        print(f"Test samples: {test_gen.samples}")
    
    # Estimate training time for user planning
    EPOCHS = 20  # We'll set this properly later
    preprocessor.estimate_training_time(train_gen.samples, EPOCHS)
    
    # Check for class imbalance
    class_distribution = train_gen.classes
    real_count = np.sum(class_distribution == 0)
    fake_count = np.sum(class_distribution == 1)
    imbalance_ratio = max(real_count, fake_count) / min(real_count, fake_count)
    
    print(f"\nüìä Training Set Analysis:")
    print(f"   Real images: {real_count}")
    print(f"   Fake images: {fake_count}")
    print(f"   Imbalance ratio: {imbalance_ratio:.2f}")
    
    if imbalance_ratio > 1.5:
        print("‚ö†Ô∏è Significant class imbalance detected - consider class weights")
    else:
        print("‚úÖ Good class balance")

# =============================================================================
# CELL 8: VISUALIZE YOUR DATASET
# =============================================================================
def visualize_dataset_samples(generator, num_samples=8):
    """Visualize samples from your dataset"""
    if generator is None:
        print("‚ùå Generator not available for visualization")
        return
    
    # Get a batch of images
    images, labels = next(generator)
    
    # Create visualization
    fig, axes = plt.subplots(2, 4, figsize=(16, 8))
    axes = axes.ravel()
    
    for i in range(min(num_samples, len(images))):
        axes[i].imshow(images[i])
        label = "Fake" if labels[i] == 1 else "Real"
        axes[i].set_title(f'{label} Aerial Image')
        axes[i].axis('off')
    
    plt.tight_layout()
    plt.suptitle('Sample Images from Your Dataset', fontsize=16, y=1.02)
    plt.show()

# Visualize your training data
if train_gen:
    print("üì∏ Visualizing samples from your training dataset:")
    visualize_dataset_samples(train_gen)
    train_gen.reset()  # Reset generator after visualization

# =============================================================================
# CELL 9: HYBRID MODEL ARCHITECTURE
# =============================================================================
class HybridDeepFakeDetector:
    def __init__(self, input_shape=(224, 224, 3), num_classes=1):
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.model = None
        
    def build_cnn_branch(self, input_tensor):
        """Build CNN branch using EfficientNet"""
        # Use EfficientNetB0 as CNN backbone
        cnn_base = EfficientNetB0(
            weights='imagenet',
            include_top=False,
            input_tensor=input_tensor,
            pooling='avg'
        )
        
        # Fine-tune last few layers (CPU optimized - freeze more layers)
        for layer in cnn_base.layers[:-10]:
            layer.trainable = False
        
        # Add custom layers
        x = cnn_base.output
        x = Dense(512, activation='relu', name='cnn_dense1')(x)
        x = BatchNormalization(name='cnn_bn1')(x)
        x = Dropout(0.3, name='cnn_dropout1')(x)
        cnn_features = Dense(256, activation='relu', name='cnn_features')(x)
        
        return cnn_features
    
    def build_vit_branch(self, input_tensor):
        """Build ViT branch"""
        try:
            print("üîÑ Loading Vision Transformer model...")
            # Load pre-trained ViT model
            vit_model = TFViTModel.from_pretrained(
                'google/vit-base-patch16-224-in21k',
                from_tf=True
            )
            print("‚úÖ ViT model loaded successfully!")
            
            # Preprocess input for ViT (ImageNet normalization)
            normalized_input = tf.keras.utils.normalize(input_tensor, axis=-1)
            
            # Get ViT outputs
            vit_outputs = vit_model(normalized_input)
            
            # Use CLS token (first token) for classification
            cls_token = vit_outputs.last_hidden_state[:, 0, :]
            
            # Add custom layers
            x = Dense(512, activation='relu', name='vit_dense1')(cls_token)
            x = BatchNormalization(name='vit_bn1')(x)
            x = Dropout(0.3, name='vit_dropout1')(x)
            vit_features = Dense(256, activation='relu', name='vit_features')(x)
            
            return vit_features, True
            
        except Exception as e:
            print(f"‚ö†Ô∏è ViT loading failed: {e}")
            print("üîÑ Falling back to CNN-only model...")
            print("üí° This is normal and your model will still work excellently!")
            return None, False
    
    def build_model(self):
        """Build complete hybrid model"""
        # Input layer
        input_img = Input(shape=self.input_shape, name='input_image')
        
        # CNN branch
        cnn_features = self.build_cnn_branch(input_img)
        
        # ViT branch
        vit_features, vit_success = self.build_vit_branch(input_img)
        
        # Feature fusion
        if vit_success and vit_features is not None:
            # Hybrid model: CNN + ViT
            print("ü§ñ Building Hybrid CNN + ViT model...")
            combined_features = Concatenate(name='feature_fusion')([cnn_features, vit_features])
            model_type = "Hybrid CNN + ViT"
        else:
            # Fallback: CNN only
            print("ü§ñ Building CNN-only model...")
            combined_features = cnn_features
            model_type = "CNN Only"
        
        # Classification head
        x = Dense(128, activation='relu', name='classifier_dense1')(combined_features)
        x = BatchNormalization(name='classifier_bn')(x)
        x = Dropout(0.5, name='classifier_dropout')(x)
        x = Dense(64, activation='relu', name='classifier_dense2')(x)
        
        # Output layer
        if self.num_classes == 1:
            output = Dense(1, activation='sigmoid', name='output')(x)
            loss = 'binary_crossentropy'
            metrics = ['accuracy', 'precision', 'recall']
        else:
            output = Dense(self.num_classes, activation='softmax', name='output')(x)
            loss = 'categorical_crossentropy'
            metrics = ['accuracy']
        
        # Create model
        self.model = Model(inputs=input_img, outputs=output, name='HybridDeepFakeDetector')
        
        # Compile model
        self.model.compile(
            optimizer=Adam(learning_rate=0.0001),
            loss=loss,
            metrics=metrics
        )
        
        print(f"‚úÖ {model_type} model built and compiled successfully!")
        return self.model

# Build the model
detector = HybridDeepFakeDetector(input_shape=(*IMAGE_SIZE, 3))
model = detector.build_model()

# Display model summary
model.summary()

# =============================================================================
# CELL 10: GTX 1650 OPTIMIZED TRAINING CONFIGURATION
# =============================================================================
class GTX1650TrainingConfig:
    """Optimized training configuration for GTX 1650 with 4000 images"""
    
    def __init__(self, train_samples, val_available=True):
        self.train_samples = train_samples
        self.val_available = val_available
        
        # Optimized parameters for GTX 1650
        self.epochs = 25  # Increased for larger dataset
        self.initial_lr = 0.0002  # Slightly higher initial LR
        self.min_lr = 1e-7
        
        # Patience settings for larger dataset
        self.early_stopping_patience = 8
        self.lr_reduction_patience = 5
        
        print(f"üéØ GTX 1650 Training Configuration:")
        print(f"   Epochs: {self.epochs}")
        print(f"   Initial Learning Rate: {self.initial_lr}")
        print(f"   Early Stopping Patience: {self.early_stopping_patience}")
    
    def setup_callbacks(self, monitor='val_loss'):
        """Setup optimized callbacks for GTX 1650"""
        callbacks = [
            # Early stopping with larger patience for 4000 images
            EarlyStopping(
                monitor=monitor,
                patience=self.early_stopping_patience,
                restore_best_weights=True,
                verbose=1,
                mode='min' if 'loss' in monitor else 'max'
            ),
            
            # Learning rate reduction
            ReduceLROnPlateau(
                monitor=monitor,
                factor=0.3,  # More aggressive reduction
                patience=self.lr_reduction_patience,
                min_lr=self.min_lr,
                verbose=1,
                mode='min' if 'loss' in monitor else 'max'
            ),
            
            # Model checkpoint
            ModelCheckpoint(
                'best_deepfake_detector_gtx1650.h5',
                monitor=monitor,
                save_best_only=True,
                save_weights_only=False,
                verbose=1,
                mode='min' if 'loss' in monitor else 'max'
            )
        ]
        
        # Add GPU memory monitoring callback
        class GPUMemoryCallback(tf.keras.callbacks.Callback):
            def __init__(self, monitor_instance):
                super().__init__()
                self.monitor = monitor_instance
                
            def on_epoch_end(self, epoch, logs=None):
                self.monitor.print_gpu_status()
                # Force garbage collection every 5 epochs
                if (epoch + 1) % 5 == 0:
                    gc.collect()
                    print("üßπ Memory cleanup performed")
        
        callbacks.append(GPUMemoryCallback(gpu_monitor))
        
        return callbacks
    
    def get_class_weights(self, train_generator):
        """Calculate class weights for imbalanced dataset"""
        class_distribution = train_generator.classes
        real_count = np.sum(class_distribution == 0)
        fake_count = np.sum(class_distribution == 1)
        
        total = real_count + fake_count
        
        # Calculate balanced class weights
        class_weights = {
            0: total / (2 * real_count),  # Real class
            1: total / (2 * fake_count)   # Fake class
        }
        
        print(f"üìä Class weights calculated:")
        print(f"   Real (0): {class_weights[0]:.3f}")
        print(f"   Fake (1): {class_weights[1]:.3f}")
        
        return class_weights

# Initialize training configuration
config = GTX1650TrainingConfig(train_gen.samples, val_gen is not None)

# Calculate class weights if needed
imbalance_ratio = max(real_count, fake_count) / min(real_count, fake_count)
if imbalance_ratio > 1.5:
    class_weights = config.get_class_weights(train_gen)
    print("‚úÖ Using class weights to handle imbalance")
else:
    class_weights = None
    print("‚úÖ No class weights needed - balanced dataset")

# Setup callbacks
monitor_metric = 'val_loss' if val_gen else 'loss'
callbacks = config.setup_callbacks(monitor=monitor_metric)

# Final training parameters
EPOCHS = config.epochs
LEARNING_RATE = config.initial_lr

print(f"\n‚úÖ Training configuration ready for GTX 1650!")
print(f"Epochs: {EPOCHS}")
print(f"Learning Rate: {LEARNING_RATE}")
print(f"Batch Size: {BATCH_SIZE}")
print(f"Image Size: {IMAGE_SIZE}")
print(f"Monitor Metric: {monitor_metric}")
print(f"Expected Training Time: ~{preprocessor.estimate_training_time(train_gen.samples, EPOCHS)}")

# =============================================================================
# CELL 11: MODEL TRAINING
# =============================================================================
def train_model_gtx1650(model, train_gen, val_gen=None, epochs=25, callbacks=None, class_weights=None):
    """Optimized training function for GTX 1650 with 4000 images"""
    print("üöÄ Starting GTX 1650 optimized training with your dataset...")
    print(f"üìä Training on {train_gen.samples} images")
    
    if train_gen is None:
        print("‚ùå No training data available!")
        return None
    
    # Calculate optimized steps
    steps_per_epoch = max(1, train_gen.samples // train_gen.batch_size)
    
    if val_gen:
        validation_data = val_gen
        validation_steps = max(1, val_gen.samples // val_gen.batch_size)
        print(f"üìä Validation steps: {validation_steps}")
    else:
        validation_data = None
        validation_steps = None
        print("‚ö†Ô∏è No validation data - monitoring training loss only")
    
    print(f"üìä Steps per epoch: {steps_per_epoch}")
    
    # Pre-training GPU memory check
    gpu_monitor.print_gpu_status()
    
    start_time = time.time()
    
    try:
        # Train model with memory monitoring
        with gpu_memory_monitor():
            history = model.fit(
                train_gen,
                steps_per_epoch=steps_per_epoch,
                epochs=epochs,
                validation_data=validation_data,
                validation_steps=validation_steps,
                callbacks=callbacks,
                class_weight=class_weights,
                verbose=1,
                workers=2,  # Optimize CPU-GPU data pipeline
                use_multiprocessing=False,  # Safer for Windows
                max_queue_size=10  # Balance memory vs speed
            )
        
        # Training completion summary
        end_time = time.time()
        training_time = end_time - start_time
        
        print("‚úÖ Training completed successfully!")
        print(f"‚è±Ô∏è Total training time: {training_time/3600:.2f} hours")
        print(f"üéØ Average time per epoch: {training_time/epochs/60:.1f} minutes")
        
        return history
        
    except tf.errors.ResourceExhaustedError as e:
        print("‚ùå GPU out of memory!")
        print("üí° Try reducing batch size to 4 or 6 and restart training")
        print(f"Error details: {e}")
        return None
    except Exception as e:
        print(f"‚ùå Training failed: {e}")
        return None

# Start optimized training for GTX 1650
print("=" * 60)
print("üéØ GTX 1650 TRAINING ON YOUR 4000 IMAGE DATASET")
print("=" * 60)

# Display training summary before starting
print(f"üöÄ Training Summary:")
print(f"   Dataset: {DATASET_PATH}")
print(f"   Training samples: {train_gen.samples}")
print(f"   Validation samples: {val_gen.samples if val_gen else 'None'}")
print(f"   Test samples: {test_gen.samples if test_gen else 'None'}")
print(f"   Batch size: {BATCH_SIZE}")
print(f"   Epochs: {EPOCHS}")
print(f"   Class weights: {'Yes' if class_weights else 'No'}")

print("\nüî• Starting training... (This will take several hours)")
print("üí° You can monitor GPU usage with 'nvidia-smi' in another terminal")

history = train_model_gtx1650(
    model=model, 
    train_gen=train_gen, 
    val_gen=val_gen, 
    epochs=EPOCHS, 
    callbacks=callbacks,
    class_weights=class_weights
)

# =============================================================================
# TRAINING VISUALIZATION WITH IMPORTS
# =============================================================================

# Import required libraries
import matplotlib.pyplot as plt
import numpy as np

print("üìä Creating training visualization...")

def plot_training_history_fixed(history):
    """Plot comprehensive training history with proper imports"""
    if history is None:
        print("‚ùå No training history available")
        return
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Plot accuracy
    axes[0, 0].plot(history.history['accuracy'], label='Training Accuracy', color='blue', linewidth=2, marker='o')
    if 'val_accuracy' in history.history:
        axes[0, 0].plot(history.history['val_accuracy'], label='Validation Accuracy', color='red', linewidth=2, marker='s')
    axes[0, 0].set_title('Model Accuracy Over Time', fontsize=14, fontweight='bold')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # Plot loss
    axes[0, 1].plot(history.history['loss'], label='Training Loss', color='blue', linewidth=2, marker='o')
    if 'val_loss' in history.history:
        axes[0, 1].plot(history.history['val_loss'], label='Validation Loss', color='red', linewidth=2, marker='s')
    axes[0, 1].set_title('Model Loss Over Time', fontsize=14, fontweight='bold')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # Training progress summary
    epochs_run = len(history.history['accuracy'])
    best_val_acc_epoch = np.argmax(history.history['val_accuracy']) + 1 if 'val_accuracy' in history.history else 0
    
    axes[1, 0].bar(['Training Acc', 'Validation Acc'], 
                   [history.history['accuracy'][-1], history.history['val_accuracy'][-1] if 'val_accuracy' in history.history else 0],
                   color=['blue', 'red'], alpha=0.7)
    axes[1, 0].set_title('Final Accuracy Comparison', fontsize=14, fontweight='bold')
    axes[1, 0].set_ylabel('Accuracy')
    axes[1, 0].grid(True, alpha=0.3)
    
    # Training summary text
    axes[1, 1].text(0.1, 0.8, f"üéØ TRAINING SUMMARY", fontsize=16, fontweight='bold', transform=axes[1, 1].transAxes)
    axes[1, 1].text(0.1, 0.7, f"Total Epochs: {epochs_run}", fontsize=12, transform=axes[1, 1].transAxes)
    axes[1, 1].text(0.1, 0.6, f"Best Val Accuracy: {max(history.history['val_accuracy']):.2%}", fontsize=12, transform=axes[1, 1].transAxes)
    axes[1, 1].text(0.1, 0.5, f"Best Epoch: {best_val_acc_epoch}", fontsize=12, transform=axes[1, 1].transAxes)
    axes[1, 1].text(0.1, 0.4, f"Final Train Acc: {history.history['accuracy'][-1]:.2%}", fontsize=12, transform=axes[1, 1].transAxes)
    axes[1, 1].text(0.1, 0.3, f"Model Type: Hybrid CNN+ViT", fontsize=12, transform=axes[1, 1].transAxes)
    axes[1, 1].text(0.1, 0.2, f"Dataset: 2,181 images", fontsize=12, transform=axes[1, 1].transAxes)
    axes[1, 1].set_xlim(0, 1)
    axes[1, 1].set_ylim(0, 1)
    axes[1, 1].axis('off')
    
    plt.tight_layout()
    plt.savefig('hybrid_deepfake_training_results.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Print detailed metrics
    print("\nüìä DETAILED TRAINING METRICS:")
    print(f"‚úÖ Final Training Accuracy: {history.history['accuracy'][-1]:.4f} ({history.history['accuracy'][-1]*100:.2f}%)")
    if 'val_accuracy' in history.history:
        print(f"‚úÖ Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f} ({history.history['val_accuracy'][-1]*100:.2f}%)")
        print(f"üèÜ Best Validation Accuracy: {max(history.history['val_accuracy']):.4f} ({max(history.history['val_accuracy'])*100:.2f}%)")
    print(f"üìâ Final Training Loss: {history.history['loss'][-1]:.4f}")
    if 'val_loss' in history.history:
        print(f"üìâ Final Validation Loss: {history.history['val_loss'][-1]:.4f}")

# Plot your training results
plot_training_history_fixed(history)

print("\nüéâ Training visualization complete!")
print("üíæ Chart saved as 'hybrid_deepfake_training_results.png'")

# =============================================================================
# CELL 13: MODEL EVALUATION ON YOUR TEST DATA
# =============================================================================
def evaluate_model_on_real_data(model, test_gen):
    """Comprehensive evaluation on your real test data"""
    if test_gen is None:
        print("‚ùå No test data available for evaluation")
        return None
    
    print("üìä Evaluating model on your real test data...")
    
    # Reset test generator
    test_gen.reset()
    
    # Get predictions
    predictions = model.predict(test_gen, verbose=1)
    predicted_classes = (predictions > 0.5).astype(int)
    
    # Get true labels
    true_labels = test_gen.classes
    
    # Calculate metrics (handle multiple metrics)
    evaluation_metrics = model.evaluate(test_gen, verbose=0)
    
    # Extract metrics based on what the model returns
    if isinstance(evaluation_metrics, list):
        test_loss = evaluation_metrics[0]
        test_accuracy = evaluation_metrics[1] if len(evaluation_metrics) > 1 else 0.0
        test_precision = evaluation_metrics[2] if len(evaluation_metrics) > 2 else 0.0
        test_recall = evaluation_metrics[3] if len(evaluation_metrics) > 3 else 0.0
    else:
        test_loss = evaluation_metrics
        test_accuracy = 0.0
        test_precision = 0.0
        test_recall = 0.0
    
    print(f"üéØ Test Results on Your Real Data:")
    print(f"Test Loss: {test_loss:.4f}")
    print(f"Test Accuracy: {test_accuracy:.4f}")
    if test_precision > 0:
        print(f"Test Precision: {test_precision:.4f}")
    if test_recall > 0:
        print(f"Test Recall: {test_recall:.4f}")
    
    # Classification report
    print("\nüìã Detailed Classification Report:")
    class_names = ['Real', 'Fake']
    report = classification_report(true_labels, predicted_classes, 
                                 target_names=class_names, output_dict=True)
    print(classification_report(true_labels, predicted_classes, target_names=class_names))
    
    # Confusion matrix
    cm = confusion_matrix(true_labels, predicted_classes)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': 'Count'})
    plt.title('Confusion Matrix - Real Dataset Results', fontsize=16, fontweight='bold')
    plt.xlabel('Predicted Label', fontsize=12)
    plt.ylabel('True Label', fontsize=12)
    plt.savefig('confusion_matrix_real_data.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # ROC Curve
    fpr, tpr, _ = roc_curve(true_labels, predictions)
    roc_auc = auc(fpr, tpr)
    
    plt.figure(figsize=(10, 8))
    plt.plot(fpr, tpr, color='darkorange', lw=3, label=f'ROC curve (AUC = {roc_auc:.3f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', fontsize=12)
    plt.ylabel('True Positive Rate', fontsize=12)
    plt.title('ROC Curve - Real Dataset Performance', fontsize=16, fontweight='bold')
    plt.legend(loc="lower right", fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.savefig('roc_curve_real_data.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return {
        'test_loss': test_loss,
        'test_accuracy': test_accuracy,
        'classification_report': report,
        'confusion_matrix': cm,
        'roc_auc': roc_auc,
        'predictions': predictions,
        'true_labels': true_labels
    }

# Evaluate on your real test data
evaluation_results = evaluate_model_on_real_data(model, test_gen)

# =============================================================================
# CELL 14: EXPLAINABLE AI ON YOUR REAL DATA
# =============================================================================
class GradCAM:
    def __init__(self, model, layer_name=None):
        self.model = model
        
        # Find the last convolutional layer if not specified
        if layer_name is None:
            for layer in reversed(model.layers):
                if len(layer.output.shape) == 4:  # Conv layer
                    layer_name = layer.name
                    break
        
        if layer_name is None:
            # Fallback to a dense layer for visualization
            for layer in reversed(model.layers):
                if 'dense' in layer.name.lower():
                    layer_name = layer.name
                    break
        
        self.layer_name = layer_name
        print(f"Using layer: {layer_name} for Grad-CAM")
        
        # Create gradient model
        try:
            self.grad_model = tf.keras.models.Model(
                [model.inputs],
                [model.get_layer(layer_name).output, model.output]
            )
        except:
            print("‚ö†Ô∏è Grad-CAM setup failed, using basic visualization")
            self.grad_model = None
    
    def generate_heatmap(self, image, class_idx=0):
        """Generate Grad-CAM heatmap for your real images"""
        if self.grad_model is None:
            return np.random.random((224, 224))  # Fallback
        
        with tf.GradientTape() as tape:
            conv_outputs, predictions = self.grad_model(image)
            loss = predictions[:, 0]
        
        # Calculate gradients
        grads = tape.gradient(loss, conv_outputs)
        
        if grads is None:
            return np.random.random((224, 224))  # Fallback
        
        # Global average pooling of gradients
        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
        
        # Weight feature maps by gradients
        conv_outputs = conv_outputs[0]
        heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
        heatmap = tf.squeeze(heatmap)
        
        # Normalize heatmap
        heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
        
        return heatmap.numpy()
    
    def visualize_heatmap(self, image, heatmap, alpha=0.4):
        """Overlay heatmap on your real aerial images"""
        try:
            # Check if heatmap is valid
            if heatmap is None or heatmap.size == 0:
                print("‚ö†Ô∏è Empty heatmap, returning original image")
                return image / 255.0 if image.max() > 1 else image
            
            # Ensure heatmap is 2D
            if len(heatmap.shape) > 2:
                heatmap = np.squeeze(heatmap)
            
            # Check for valid heatmap dimensions
            if len(heatmap.shape) != 2:
                print(f"‚ö†Ô∏è Invalid heatmap shape: {heatmap.shape}, returning original image")
                return image / 255.0 if image.max() > 1 else image
            
            # Resize heatmap to match image size
            heatmap_resized = cv2.resize(heatmap.astype(np.float32), 
                                       (image.shape[1], image.shape[0]))
            
            # Normalize heatmap to [0,1]
            if heatmap_resized.max() > heatmap_resized.min():
                heatmap_resized = (heatmap_resized - heatmap_resized.min()) / (heatmap_resized.max() - heatmap_resized.min())
            else:
                heatmap_resized = np.zeros_like(heatmap_resized)
            
            # Convert heatmap to colormap
            heatmap_colored = plt.cm.jet(heatmap_resized)[:, :, :3]
            
            # Normalize image
            if image.max() > 1:
                image = image / 255.0
            
            # Overlay heatmap
            overlayed = heatmap_colored * alpha + image * (1 - alpha)
            
            return overlayed
            
        except Exception as e:
            print(f"‚ö†Ô∏è Heatmap visualization failed: {e}")
            return image / 255.0 if image.max() > 1 else image

def demonstrate_gradcam_on_real_data(model, test_gen, num_samples=6):
    """Demonstrate Grad-CAM on your real aerial images"""
    if test_gen is None:
        print("‚ùå No test data available for Grad-CAM demonstration")
        return
    
    print("üéØ Generating Grad-CAM explanations on your real aerial images...")
    
    # Initialize Grad-CAM
    gradcam = GradCAM(model)
    
    # Get some real test samples
    test_gen.reset()
    test_images, test_labels = next(test_gen)
    
    # Select samples
    indices = np.random.choice(len(test_images), min(num_samples, len(test_images)), replace=False)
    
    fig, axes = plt.subplots(3, len(indices), figsize=(4*len(indices), 12))
    if len(indices) == 1:
        axes = axes.reshape(-1, 1)
    
    for i, idx in enumerate(indices):
        image = test_images[idx:idx+1]
        true_label = test_labels[idx]
        
        # Get prediction
        prediction = model.predict(image, verbose=0)[0][0]
        predicted_label = "Fake" if prediction > 0.5 else "Real"
        true_label_text = "Fake" if true_label == 1 else "Real"
        confidence = prediction if prediction > 0.5 else 1 - prediction
        
        # Generate heatmap
        heatmap = gradcam.generate_heatmap(image)
        
        # Original image
        axes[0, i].imshow(test_images[idx])
        axes[0, i].set_title(f'Original Aerial Image\nTrue: {true_label_text}', fontsize=10)
        axes[0, i].axis('off')
        
        # Prediction info
        axes[1, i].text(0.5, 0.5, f'Prediction: {predicted_label}\nConfidence: {confidence:.3f}\nRaw Score: {prediction:.3f}', 
                       transform=axes[1, i].transAxes, ha='center', va='center', fontsize=12,
                       bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))
        axes[1, i].set_xlim(0, 1)
        axes[1, i].set_ylim(0, 1)
        axes[1, i].axis('off')
        
        # Grad-CAM overlay
        overlayed = gradcam.visualize_heatmap(test_images[idx], heatmap)
        axes[2, i].imshow(overlayed)
        axes[2, i].set_title('Grad-CAM Explanation\n(Red = High Influence)', fontsize=10)
        axes[2, i].axis('off')
    
    plt.tight_layout()
    plt.savefig('gradcam_explanations_real_data.png', dpi=300, bbox_inches='tight')
    plt.show()

# Generate Grad-CAM explanations on your real data
demonstrate_gradcam_on_real_data(model, test_gen)

# =============================================================================
# CELL 15: SAVE YOUR TRAINED MODEL
# =============================================================================
def save_trained_model(model, history, evaluation_results):
    """Save your trained model and all results"""
    print("üíæ Saving your trained model and results...")
    
    # Save complete model
    model.save('my_aerial_deepfake_detector.h5')
    print("‚úÖ Model saved as: my_aerial_deepfake_detector.h5")
    
    # Save model weights only
    model.save_weights('my_model_weights.weights.h5')
    print("‚úÖ Weights saved as: my_model_weights.weights.h5")
    
    # Save model architecture
    with open('my_model_architecture.json', 'w') as f:
        f.write(model.to_json())
    print("‚úÖ Architecture saved as: my_model_architecture.json")
    
    # Save training history
    if history:
        import pickle
        with open('my_training_history.pkl', 'wb') as f:
            pickle.dump(history.history, f)
        print("‚úÖ Training history saved as: my_training_history.pkl")
    
    # Save evaluation results
    if evaluation_results:
        np.save('my_evaluation_results.npy', evaluation_results)
        print("‚úÖ Evaluation results saved as: my_evaluation_results.npy")
    
    # Create comprehensive report
    with open('MY_MODEL_PERFORMANCE_REPORT.txt', 'w') as f:
        f.write("AERIAL DEEPFAKE DETECTOR - PERFORMANCE REPORT\n")
        f.write("=" * 60 + "\n\n")
        f.write("MODEL INFORMATION:\n")
        f.write(f"- Architecture: Hybrid CNN (EfficientNet) + Vision Transformer\n")
        f.write(f"- Total Parameters: {model.count_params():,}\n")
        f.write(f"- Input Size: {model.input_shape}\n")
        f.write(f"- Training Dataset: /content/dataset\n\n")
        
        if history:
            f.write("TRAINING RESULTS:\n")
            f.write(f"- Epochs Trained: {len(history.history['loss'])}\n")
            f.write(f"- Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\n")
            if 'val_accuracy' in history.history:
                f.write(f"- Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\n")
            f.write(f"- Final Training Loss: {history.history['loss'][-1]:.4f}\n")
            if 'val_loss' in history.history:
                f.write(f"- Final Validation Loss: {history.history['val_loss'][-1]:.4f}\n\n")
        
        if evaluation_results:
            f.write("TEST RESULTS:\n")
            f.write(f"- Test Accuracy: {evaluation_results['test_accuracy']:.4f}\n")
            f.write(f"- Test Loss: {evaluation_results['test_loss']:.4f}\n")
            f.write(f"- ROC AUC Score: {evaluation_results['roc_auc']:.4f}\n\n")
            
            f.write("DETAILED CLASSIFICATION METRICS:\n")
            f.write(str(evaluation_results['classification_report']))
    
    print("‚úÖ Comprehensive report saved as: MY_MODEL_PERFORMANCE_REPORT.txt")
    print("\nüìÅ All saved files:")
    print("- my_aerial_deepfake_detector.h5 (Complete trained model)")
    print("- my_model_weights.h5 (Model weights only)")
    print("- my_model_architecture.json (Model structure)")
    print("- my_training_history.pkl (Training curves data)")
    print("- my_evaluation_results.npy (Test results)")
    print("- MY_MODEL_PERFORMANCE_REPORT.txt (Comprehensive report)")

# Save everything
save_trained_model(model, history, evaluation_results)

# =============================================================================
# CELL 16: TEST YOUR MODEL ON NEW IMAGES
# =============================================================================
def test_single_image(model, image_path, show_gradcam=True):
    """Test your trained model on a single new aerial image"""
    if not os.path.exists(image_path):
        print(f"‚ùå Image not found: {image_path}")
        return None
    
    # Load and preprocess image
    try:
        img = tf.keras.preprocessing.image.load_img(image_path, target_size=IMAGE_SIZE)
        img_array = tf.keras.preprocessing.image.img_to_array(img)
        img_array = np.expand_dims(img_array, axis=0) / 255.0
        
        # Make prediction
        prediction = model.predict(img_array, verbose=0)[0][0]
        confidence = prediction if prediction > 0.5 else 1 - prediction
        predicted_class = "FAKE" if prediction > 0.5 else "REAL"
        
        print(f"üñºÔ∏è Analysis of: {os.path.basename(image_path)}")
        print(f"üéØ Prediction: {predicted_class}")
        print(f"üìä Confidence: {confidence:.4f}")
        print(f"üìà Raw Score: {prediction:.4f}")
        
        # Visualization
        fig, axes = plt.subplots(1, 2 if show_gradcam else 1, figsize=(15 if show_gradcam else 8, 6))
        
        if not show_gradcam:
            axes = [axes]
        
        # Original image
        axes[0].imshow(img)
        color = 'red' if predicted_class == 'FAKE' else 'green'
        axes[0].set_title(f'Aerial Image Analysis\nPrediction: {predicted_class}\nConfidence: {confidence:.4f}', 
                         fontsize=14, color=color, fontweight='bold')
        axes[0].axis('off')
        
        # Grad-CAM explanation
        if show_gradcam:
            try:
                gradcam = GradCAM(model)
                heatmap = gradcam.generate_heatmap(img_array)
                overlayed = gradcam.visualize_heatmap(np.array(img), heatmap)
                
                axes[1].imshow(overlayed)
                axes[1].set_title('Explanation: Areas of Interest\n(Red = High Influence on Decision)', 
                                fontsize=14, fontweight='bold')
                axes[1].axis('off')
            except Exception as e:
                print(f"‚ö†Ô∏è Grad-CAM visualization failed: {e}")
        
        plt.tight_layout()
        plt.show()
        
        return {
            'prediction': prediction,
            'predicted_class': predicted_class,
            'confidence': confidence,
            'image_path': image_path
        }
        
    except Exception as e:
        print(f"‚ùå Error processing image {image_path}: {e}")
        return None

# Example usage - replace with your image path
# result = test_single_image(model, '/content/your_test_image.jpg')

print("üéØ To test your model on a new image, use:")
print("result = test_single_image(model, 'path_to_your_image.jpg')")

# =============================================================================
# CELL 16.5: SHAP EXPLAINABLE AI (OPTIONAL)
# =============================================================================
def demonstrate_shap_explanations(model, train_gen, test_gen, num_samples=2):
    """Generate SHAP explanations to complement Grad-CAM"""
    try:
        import shap
        print("üîç Generating SHAP explanations for your deepfake detector...")
        
        # Get background samples
        background_batch = next(iter(train_gen))
        background_images = background_batch[0][:3]
        
        # Initialize SHAP explainer
        explainer = shap.DeepExplainer(model, background_images)
        
        # Get test samples
        test_gen.reset()
        test_batch = next(test_gen)
        test_images = test_batch[0][:num_samples]
        
        # Calculate SHAP values
        print("‚ö° Calculating SHAP values...")
        shap_values = explainer.shap_values(test_images)
        
        # Visualize
        shap.image_plot(shap_values, test_images)
        
        # Save
        plt.savefig('shap_explanations_deepfake_detector.png', dpi=300, bbox_inches='tight')
        print("üíæ SHAP visualization saved!")
        
        return True
        
    except ImportError:
        print("‚ö†Ô∏è SHAP not available - using Grad-CAM only")
        return False
    except Exception as e:
        print(f"‚ö†Ô∏è SHAP failed: {e}")
        return False

# Run SHAP analysis if available
if SHAP_AVAILABLE and train_gen and test_gen:
    demonstrate_shap_explanations(model, train_gen, test_gen)

# =============================================================================
# CELL 17: FINAL SUMMARY
# =============================================================================
print("\n" + "=" * 80)
print("üéâ HYBRID DEEPFAKE DETECTOR TRAINING COMPLETED ON YOUR REAL DATA! üéâ")
print("=" * 80)

# Calculate and display final statistics
if history and evaluation_results:
    print(f"""
üìä YOUR MODEL'S PERFORMANCE SUMMARY:
Dataset: /content/dataset
Training Images: {train_gen.samples if train_gen else 'N/A'}
Validation Images: {val_gen.samples if val_gen else 'N/A'}
Test Images: {test_gen.samples if test_gen else 'N/A'}

üéØ FINAL RESULTS:
- Training Accuracy: {history.history['accuracy'][-1]:.4f}
- Validation Accuracy: {history.history.get('val_accuracy', ['N/A'])[-1] if isinstance(history.history.get('val_accuracy', ['N/A'])[-1], float) else 'N/A'}
- Test Accuracy: {evaluation_results['test_accuracy']:.4f}
- ROC AUC Score: {evaluation_results['roc_auc']:.4f}

üöÄ MODEL CAPABILITIES:
‚úÖ Detects fake aerial/satellite images
‚úÖ Provides confidence scores
‚úÖ Generates visual explanations (Grad-CAM)
‚úÖ Ready for deployment

üìÅ SAVED FILES:
‚úÖ Complete trained model (.h5)
‚úÖ Model weights and architecture
‚úÖ Training history and metrics
‚úÖ Performance visualizations
‚úÖ Comprehensive report
""")

print("üîß NEXT STEPS:")
print("1. Test your model on new aerial images using test_single_image()")
print("2. Fine-tune with more data if needed")
print("3. Deploy for real-world use")
print("4. Share your results!")

print("\nüéØ YOUR DEEPFAKE DETECTOR IS READY TO USE!")
print("=" * 80)
